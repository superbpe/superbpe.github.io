<!DOCTYPE html>
<html>
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-M2MCV7MZ');</script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="description"
        content="SuperBPE: Space Travel for Language Models">
  <meta name="keywords" content="superbpe superword tokenization">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SuperBPE: Space Travel for Language Models</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://cdn.jsdelivr.net/npm/@iframe-resizer/parent@5.3.3"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-M2MCV7MZ"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SuperBPE: Space Travel for Language Models</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">*<a href="https://alisawuffles.github.io/">Alisa Liu</a><sup>&hearts;&#65038;&spades;&#65038;</sup>,</span>
            <span class="author-block">*<a href="https://jon.jon.ke/">Jonathan Hayase</a><sup>&hearts;&#65038;</sup>,</span>
            <span class="author-block"><a href="https://valentinhofmann.github.io/">Valentin Hofmann</a><sup>&diams;&#65038;&hearts;&#65038;</sup>,</span>
            <span class="author-block"><a href="https://homes.cs.washington.edu/~sewoong/">Sewoong Oh</a><sup>&hearts;&#65038;</sup>,</span>
            <span class="author-block"><a href="https://nasmith.github.io/">Noah A. Smith</a><sup>&hearts;&#65038;&diams;&#65038;</sup>,</span>
            <span class="author-block"><a href="https://homes.cs.washington.edu/~yejin/">Yejin Choi</a><sup>&spades;&#65038;</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>&hearts;&#65038;</sup>University of Washington,</span>
            <span class="author-block"><sup>&spades;&#65038;</sup>NVIDIA</span>,
            <span class="author-block"><sup>&diams;&#65038;</sup>Allen Institute for AI</span>,
            <span class="eql-cntrb">*Equal contribution</span>
          </div>
        </div>
      </div>
      
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- PDF Link. -->
            <span class="link-block">
              <a href="https://arxiv.org/pdf/2503.13423" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://arxiv.org/abs/2503.13423" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>arXiv</span>
              </a>
            </span>
            <!-- Code Link. -->
            <span class="link-block">
              <a href="coming-soon.txt" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://huggingface.co/collections/UW/superbpe-67db2338062faa07c7473ffa" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-cloud-download-alt"></i></span>
                <span>HuggingFace</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="tldr">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
          <!-- TL;DR Section -->
        <div class="content is-size-5">
          <em><strong>TL;DR:</strong> We introduce a family of superword tokenizers which encode the same text using, on average, up to 33% fewer tokens than a BPE tokenizer of the same size. Models trained with our tokenizer are more efficient during inference while also outperforming the baseline by +4.0% on average over a suite of 30 downstream tasks, including +8.2% on MMLU.</em>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="Playground">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="content">
          <h2 class="title">Playground</h2>
          <iframe src="https://jhayase-superbpe-tokenizer-playground.static.hf.space"
                  frameborder="0"
            width="100%"
            id="#playgroundframe"></iframe>
    <script type="module">
      import { initialize } from "https://cdn.jsdelivr.net/npm/@open-iframe-resizer/core@latest/dist/index.js";

      initialize();
    </script>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <div class="content">
            <h2 class="title">BibTeX</h2>
            <pre><code>@misc{liu2025superbpespacetravellanguage,
      title={SuperBPE: Space Travel for Language Models},
      author={Alisa Liu and Jonathan Hayase and Valentin Hofmann and Sewoong Oh and Noah A. Smith and Yejin Choi},
      year={2025},
      eprint={2503.13423},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2503.13423},
}</code></pre>
          </div>
        </div>
      </div>
    </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://advtok.github.io/">Adversarial Tokenization project page</a>.
            The playground is based on <a href="https://huggingface.co/spaces/Xenova/the-tokenizer-playground/">Xenova/the-tokenizer-playground</a>.
            This website is licensed under <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a>.
            You are free to borrow the <a href="https://github.com/superbpe/superbpe.github.io">source code</a> of this website, we just ask that you remove the analystics code in the header and link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
